---
title: "Spike-and-Slab Additive Models And Fast Algorithms For High-Dimensional Data Analysis"
author: "Boyi Guo"
institute: |
  | Department of Biostatistics
  | University of Alabama at Birmingham
date: "July 12th, 2022"
output: 
  beamer_presentation:
    keep_tex: true
    theme: "Szeged"
    colortheme: "spruce"
    toc: FALSE
    number_section: false
    slide_level: 3
    includes:
      in_header: "preamble.tex"
classoption: "aspectratio=169"
bibliography:
  - bibfile.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
## target knits Rmds in their own session, so load libraries here.
# source("here::here(packages.R)")
```



# Outline {.unlisted .unnumbered}

* Background
    * Spline Model Development
    * Bayesian Regularization
    * Bayesian Variable Selection
* Dissertation
    * Two-part Spike-and-slab LASSO Prior for Spline Functions
    * Fast and Scalable Model Fitting Algorithms
    * Empirical Performance of Prediction & Selection
* Future Research
    * Structured Additive Regression with Spike-and-Slab LASSO prior
    * Spatially Variable Genes Screening
    * Other Questions of Interest

# Background

## Spline Model Development



### Nonlinear Effect Modeling

> "It is extremely unlikely that the true (effect) function f(X) (_on the outcome_) is actually linear in X."
\hspace*{2cm}

> --- @hastie2009elements PP. 139

* Traditional modeling approaches
  * Categorization of continuous variable, polynomial regression
  * Simple but may be statistically flawed

* Machine learning methods
  * Black-box algorithms: Random forests, neural network
  * Predict accurate but too complicated for interpretation

### Generalized Additive Model (GAM)
Firstly formulated by @Hastie1987
\begin{align*}
  y_i \simiid EF(\mu_i, \phi), \quad i = 1, \dots, n\\
  \mu_i = g^{-1}(\beta_0 + \sum\limits^p_{j=1}B_j(x_j))
\end{align*}
where $B_j(x_j)$ is a smoothing function, $g(\cdot)$ is a link function, $\phi$ is the dispersion parameter

* Objective: to estimate smoothing functions $B_j(x_j)$
* Applications in biomedical research:
  * Dose-response curve
  * Time-varying effect

## Bayesian Regularization

## Bayesian Variable Selection

### High-dimensional GAM
* Grouped penalty models
  * Grouped lasso penalty [@Ravikumar2009; @Huang2010], grouped SCAD penalty [@Wang2007; @Xue2009]
  * Sparse penalty induces __excess shrinkage__, causing inaccurate interpolation of nonlinear effect

* Bayesian Hierarchical Models
  * Grouped spike-and-slab priors [@Scheipl2012; @Yang2020], grouped spike-and-slab lasso prior[@Bai2020; @Bai2021]
  * Mostly Markov chain Monte Carlo methods for model fitting
  * Computational inefficiency causes __scaling problems__ in high-dimensional data analysis
  
### Other challenges
* Bi-level selection
  * To detect if a smoothing function is linear and nonlinear
  * All-in-all-out selection reduces the ability of result interpretation
  
* Uncertainty inferences
  * Penalized models doesn't provide uncertainty measures
  * Challenging to estimate the effective degree of freedom for each smoothing functions




# Dissertation
Scope of this dissertation

* BHAM
* Survival Model
* R package `BHAM`

## Objectives
* To develop statistical models that improve curve interpolation and outcome prediction
  * Local adaption of sparse penalty and smooth penalty
  * Bi-level selection for linear and nonlinear effect
* To develop a fast and scalable algorithm
* To implement a user-friendly statistical software

### Model
Given the data $\{\bm X_i, y_i\}_{i=1}^n$ where $\bm X_i \in \mathbb{R}^p$, $y_i \in \mathbb{R}$ and $p >> n$, we have the generalized additive model
\begin{align*}
y_i &\overset{\text{i.i.d.}}{\sim} EF(\mu_i, \phi),\\
g(\mu_i) &= g^{-1}(\beta_0 + \sum\limits^p_{j=1}B_j(x_j)) , \quad i = 1, \dots, n.
\end{align*}
The smoothing function can be written in a matrix form $B_j(x_j) = \bs \beta_j^T \bs X_j$, where $\bs \beta_j$ are the coefficients of the smoothing function and $\bs X_j$ is the basis matrix of dimension $K_j$.

<!-- We express smoothing functions in the matrix form: linear component $X_{ij}^0$, nonlinear component $X_{ij}^{pen}$ and their coefficients $\beta_{j}^0, \beta^{pen}_j$ respectively. -->
<!-- $$ -->
<!-- g(\mu_i) = \sum\limits_{j=1}^p f_j(X_{ij}) = \sum\limits_{j=1}^p\left[{\beta_j^0}^T X_{ij}^0 + {\beta_j^{pen}}^T X_{ij}^{pen}\right]. -->
<!-- $$ -->

### Smoothing Function Reparameterization
* Smoothing penalty from Smoothing spline regression [@Wood2017]
$$
\lambda_j \int B^{\prime\prime}_j(x)dx = \lambda_j \bs \beta_j^T \bs S_j \bs \beta_j,
$$
where $S_j$ is a known smoothing penalty matrix.

* Isolate the linear and nonlinear components via eigendecomposing $S_j$
  <!-- * $\bm S = \bm U \bm D \bm U^T$ -->
  <!-- * $\bm U \equiv [\bm U^\tp : \bm U^0]$ and $\bm D \equiv [\bm D^\tp : \bm 0]$ -->
   $$\bm X \bm \beta = X^{0} \beta + \bs X^\tp\beta^\tp$$

* Benefits
  * Motivate bi-level selection
  * Implicit modeling of function smoothness
  * Reduce computation load with conditionally independent prior of basis coefficients 


### Two-part Spike-and-slab LASSO (SSL) Prior

* SSL prior for the linear coefficient and group SSL priors for nonlinear coefficients
\begin{align*}
\beta_{j} |\gamma_{j},s_0,s_1 &\sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1) \\
\beta^\tp_{jk} | \gamma^\tp_{j},s_0,s_1 &\simiid DE(0,(1-\gamma^\tp_{j}) s_0 + \gamma^\tp_{j} s_1), k = 1, \dots, K_j
\end{align*}
* Effect hierarchy enforced latent inclusion indicators $\gamma_j$ and $\gamma^\tp_{j}$ for bi-level selection
$$
\gamma_{j} | \theta_j \sim Bin(\gamma_{j}|1, \theta_j),\quad
\gamma_{j}^\tp | \bg{\gamma_{j}}, \theta_j \sim Bin(1, \bg{\gamma_{j}}\theta_j),
$$
* Local adaptivity of signal sparsity and function smoothness
$$
\theta_j \sim \text{Beta}(a,b)
$$

### Visual Representation
\begin{figure}
\centering
\resizebox{12cm}{5.5cm}{
\begin{tikzpicture} [
staticCompo/.style = {rectangle, minimum width=1cm, minimum height=1cm,text centered, draw=black, fill=blue!30},
outCome/.style={ellipse, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=blue!30},
mymatrix/.style={matrix of nodes, nodes=outCome, row sep=1em},
PriorBoarder/.style={rectangle, minimum width=5cm, minimum height=10cm, text centered, fill=lightgray!30},
background/.style={rectangle, fill=gray!10,inner sep=0.2cm, rounded corners=5mm}
]

\matrix (linearPrior) [matrix of nodes, column sep = 0mm, row sep = 0.7cm] {
  \node (linearGamma) [outCome] { $\gamma_j \sim Bin(1, \theta_j) $ };\\
  \node (linearBeta) [outCome] { $\beta_j \sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1)$};\\
};
\matrix (penPrior) [right = 2cm of linearPrior, matrix of nodes, column sep = 0mm, row sep = 0.7cm] {
  \node (penGamma) [outCome] { $\gamma_{j}^\tp \sim Bin(1, \bg{\gamma_{j}}\theta_j)$ };\\
  \node (penBeta) [outCome] { $\beta_{jk}^\tp \sim  DE(0,(1-\gamma^\tp_{j}) s_0 + \gamma^\tp_{j} s_1)$};\\
};


\node (s) [staticCompo]  at ($(linearBeta)!0.5!(penBeta)$)  {($s_0, s_1$)};
\node (Beta) [staticCompo, below = 1cm of s] {$\bs \beta = (\beta_1, \bs \beta^\tp_1, \dots,\beta_j, \bs \beta^\tp_j , \dots,\beta_p, \bs \beta^\tp_p) $};
\node (Theta)[outCome, above = 2cm of s] {$\theta_{j} \sim Beta(a, b)$};
\node (ab)[staticCompo, above = 0.5cm of Theta] {$(a, b)$};
\node (Y) [outCome, below = 1cm of Beta] {$y_i \sim Expo. Fam. (g^{-1}(\bs \beta^T \bs X_i), \phi)$};

\draw[->] (Theta) -- (linearGamma);
\draw[->] (Theta) -- (penGamma);
\draw[->] (linearGamma) -- (linearBeta) ;
\draw[->] (penGamma) -- (penBeta);
\draw[->, draw = red] (linearGamma) -- (penGamma);
\draw[->] (ab) -- (Theta);
\draw[->] (s) -- (linearBeta) ;
\draw[->] (s) -- (penBeta);
\draw[->] (linearBeta) -- (Beta);
\draw[->] (penBeta) -- (Beta);
\draw[->] (Beta) --  (Y);


\begin{pgfonlayer}{background}
  \node [background,
   fit=(linearGamma) (linearBeta),
   label=above:Linear Space:] {};
  \node [background,
    fit=(penGamma) (penBeta),
    label=above:Nonlinear Space:] {};
\end{pgfonlayer}

\end{tikzpicture}
}
\end{figure}


## EM-Cooridante Descent Algrithm for Scalable Model Fitting

We are interested in estimating $\Theta = \{\bm \beta, \bm \theta, \phi\}$ using optimization based algorithm for scalability purpose

* Basic Ideas
  * Treat $\gamma$s as the "missing data" in the EM procedure
  * Quantify the expectation of log posterior density function of $\Theta$ with respect to $\gamma$ conditioning on $\Theta^{(t-1)}$
  * Maximize two  parts of the objective function independently
  
* Previous applications in high-dimensional data analysis
  * EMVS [@Rockova2014a], Spike-and-slab lasso [@Rockova2018]
  * BhGLM [@Yi2019]



### Decomposition of Objective Function
We aim to maximize the log posterior density of $\Theta$ by averaging over all possible values of $\bm \gamma$
$$ \log f(\Theta, \bs \gamma| \textbf{y}, \textbf{X}) = Q_1(\bs \beta, \phi) + Q_2 (\bs \gamma,\bs \theta),$$

* L$_1$-penalized likelihood function of $\bs \beta, \phi$
$$Q_1 \equiv Q_1(\bs \beta, \phi) = \log f(\textbf{y}|\bs \beta, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|\gamma_j)+\sum\limits_{k=1}^{K_j} \log f(\beta^{\tp}_{jk}|\gamma^{\tp}_{jk})\right]$$
* Posterior density of $\theta$ given data points $\gamma$s
$$Q_2 \equiv Q_2(\bs\gamma,\bs\theta) = \sum\limits_{j=1}^{p} \left[ (\gamma_j+\gamma_{j}^{\tp})\log \theta_j + (2-\gamma_j-\gamma_{j}^{\tp}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log f(\theta_j).$$

* $Q_1$ and $Q_2$ are independent conditioning on $\gamma$s


### Summary of EM-Coordinate Descent Algorithm
* E-step
  * Formulate $E_{\bm \gamma|\Theta^{(t)}}\left[Q(\Theta, \bm \gamma)\right] = E(Q_1) + E(Q_2)$
    * $E(Q_1)$ is a penalized likelihood function of $\beta, \phi$
    * $E(Q_2)$ is a posterior density of $\theta$ given $E(\gamma)$
    * $E(Q_1)$ and $E(Q_2)$ are conditionally independent
  * Calculate $E(\gamma_{j})$, $E(\gamma^\tp_{j})$ and the penalties parameters by Bayes' theorem
* M-step: 
  * Use Coordinate Descent to fit the penalized model in $E(Q_1)$ to update $\beta, \phi$ 
  * Closed form calculation via $E(Q_2)$ to update $\theta$

### Tuning Parameter Selection
* $s_0$ and $s_1$ are tuning parameters
* Empirically, $s_1$ has extremely small effect on changing the estimates
* Focus on tuning $s_0$
* Consider a sequence of $L$ ordered values $\{s_0^l\}: 0 < s_0^1 < s_0^2 < \dots < s_0^L < s_1$
* Cross-validation to choose optimal value for $s_0$

## Simulation Study
* Follow the data generating process introduced in @Bai2020. 
* $n_{train} = 500$, $n_{test}=1000$
* $p=4, 10, 50, 200$
$$
\mu = 5 \sin(2\pi x_1) - 4 \cos(2\pi x_2 -0.5) + 6(x_3-0.5) - 5(x_4^2 -0.3),
$$

* $f_j(x_j) = 0$ for $j = 5, \dots, p$.
* 2 types of outcome: Gaussian ($\phi=1$), Binomial
* Splines are constructed using 10 knots
* 50 Iterations

### Comparison & Metircs
* Methods of comparison
  * Proposed model BHAM
  * Linear LASSO model as the benchmark
  * mgcv [@Wood2004]
  * COSSO [@Zhang2006GAM] and adaptive COSSO[@Storlie2011]
  * Sparse Bayesian GAM [@Bai2021]
  * spikeSlabGAM [@Scheipl2012]

* Metrics
  * Prediction: $R^2$ for continuous outcomes, out-of-sample AUC for binary outcomes
  * Variable Selection: positive predictive value (precision), true positive rate (recall), and Matthews correlation coefficient (MCC)
  
### Prediction Performance

* Linear LASSO Model performs bad and mgcv performs well
* BHAM performs better than COSSO, adaptive COSSO and spikeSlabGAM
* BHAM performs better than SB-GAM in low-dimensional case but slightly worse in the high-dimensional setting
* BHAM is much faster than SB-GAM in fitting models

### Variable Selection Performance
* SB-GAM has the best variable selection performance
* BHAM has conservative selection
* BHAM and spikeSlabGAM have trade-offs for bi-level selection
  * spikeSlabGAM tends to select either linear or nonlinear components of the funciton
  * BHAM is more likely to select both parts



  
# Conclusion 

- Propose a scalable Bayesian Hierarchical Additive Model (BHAM) for high-dimensional data analysis
  - Organic balance between sparse penalty and smooth penalty
  - Bi-level selection for linear and nonlinear effects

- R package: \texttt{BHAM}
  - Ancillary functions for high-dimensional formulation
  - Model summary and variable selection
  - Website via [_boyiguo1.github.io/BHAM_](https://boyiguo1.github.io/BHAM/)
  
  
# Acknowledgement {.unlisted .unnumbered}

# References {.allowframebreaks}
