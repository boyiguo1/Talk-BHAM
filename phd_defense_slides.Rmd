---
title: "Spike-and-Slab Additive Models And Fast Algorithms For High-Dimensional Data Analysis"
author: "Boyi Guo"
institute: |
  | Department of Biostatistics
  | University of Alabama at Birmingham
date: "July 12th, 2022"
output: 
  beamer_presentation:
    keep_tex: true
    theme: "Szeged"
    colortheme: "spruce"
    toc: FALSE
    number_section: false
    slide_level: 3
    includes:
      in_header: "preamble.tex"
classoption: "aspectratio=169"
bibliography:
  - bibfile.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
## target knits Rmds in their own session, so load libraries here.
# source("here::here(packages.R)")
```



# Outline {.unlisted .unnumbered}

* Background
    * Spline Model Development
    * Bayesian Regularization
    * Bayesian Variable Selection
* Dissertation
    * Two-part Spike-and-slab LASSO Prior for Spline Functions
    * EM-Coordinate Descent Algorithms
    * Empirical Performance of Prediction & Selection
* Future Research
    * Structured Additive Regression with Spike-and-Slab LASSO prior
    * Spatially Variable Genes Screening
    * Other Questions of Interest

# Background

## Spline Model Development

> "It is extremely unlikely that the true (effect) function f(X) (_on the outcome_) is actually linear in X."
\hspace*{2cm}

> --- @hastie2009elements PP. 139

* Traditional modeling approaches
  * Categorization of continuous variable, polynomial regression
  * Simple but may be statistically flawed

* Machine learning methods
  * Black-box algorithms: Random forests, neural network
  * Predict accurate but too complicated for interpretation

### Spline Functions

::: columns
:::: column

A _spline_ function is a piece-wise polynomial function
$$
B(x) = \sum\limits_{k = 1}^K \beta_k b_k(x) \equiv  \bs X^T \bs \beta
$$
$b_k(x)$ are the _basis functions_, possibly truncated power basis and b-spline basis.
::::

:::: column
\centering
![A cubic spline function with 2 knots (courtesy of @hastie2009elements)](spline_visual.jpg){height=60%}
::::
:::

We defer more information about spline functions to @Wood2017

We assume the knots of the functions are equidistance.


### Generalized Additive Models with Splines

__Generalized additive model__ [@Hastie1987] is expressed
\begin{align*}
  y_i &\simiid EF(\mu_i, \phi), \quad i = 1, \dots, n\\
  g(\mu_i) &= \beta_0 + B(x_i) = \beta_0 + \bs X_i^T \bs \beta ,  \quad \mathbb{E}\left[B(X)\right] = 0 
\end{align*}
where $B(x_i)$ is the spline function, $g(\cdot)$ is a link function, $\phi$ is the dispersion parameter

\vspace*{0.5cm}

* Model fitting follows the generalized linear models, e.g. ordinary least square for Gaussian outcome
$$
\boldsymbol{\hat \beta} = \text{arg}\min \sum\limits^n_{i=1} \left[y_i - \beta_0 - \bs X_i^T \bs \beta \right]^2
$$

### Problem: Function Smoothness

The estimation of $B(X)$ can be wiggly when the underlying function is smooth, particularly as the number of bases ,$K$, increases. 

[TODO: add two plots, overfitting and not overfitting]

## Bayesian Regularization

### Smoothing Spline Model

* Smoothing penalty $\lambda \int B^{''}(X)^2dx = \lambda \bs \beta^T \bs S \bs\beta$
  * The smoothing penalty matrix $\bs S$ is known given $\bs X$
  * $\bs S$ is symmetric and positive semi-definite

* Penalized Least Square for Gaussian Outcome
$$
\boldsymbol{\hat \beta} = \text{arg}\min \sum\limits^n_{i=1} \sum\limits^n_{i=1} \left[y_i - \beta_0 - \bs X_i^T \bs \beta \right]^2 + \lambda \bs \beta^T \bs S \bs\beta
$$

* The smoothing parameter $\lambda$ is a tuning parameter, selected via cross-validation

### Problem: Multiple Predictor Model

When a model contains multiple spline functions for variables $X_1, \dots, X_p$, the penalized least square estimator is
$$
\boldsymbol{\hat \beta} = \text{arg}\min \sum\limits^n_{i=1} \sum\limits^n_{i=1} \left[y_i - \beta_0 - \sum\limits \bs X_{ij}^T \bs \beta_j \right]^2 + \lambda_j \bs \beta_j^T \bs S_j \bs\beta_j
$$

_How to decide $\lambda_i$?_

* Global smoothing, i.e. $\lambda_1 = \cdots =\lambda_p$ assumes all functions shares the same shape
* Adaptive smoothing, i.e. examining $\lambda_i$ combination, are computationally intensive

### Bayesian Regularization

* Bayesian Regularization is the Bayesian analogy of penalized models by using regularizing priors
  * Bayesian ridge via normal prior
  $$
   \beta \sim N(0, \tau^2) \rightarrow  \lambda = \sigma^2/\tau^2
  $$
  
* Adaptive shrinkage with hierarchical priors
  $$
   \tau^2_j \simiid IG(a, b)
  $$
  * Adaptive Smoothing
    * Random walk prior on b-spline bases with IG hyperprior
    * Normal prior on truncated power bases with a log-normal spline model for variance

## Bayesian Variable Selection
### Problem: Functional Selection
In the context of variable selection and high-dimensional statistics, we always assume some variables are not effective or predictive to the outcome.

How to statistically detect

* if a variable is predictive to the outcome, $B_j(X_j) = 0$
* if a variable has a nonlinear relationship with the outcome, $B_j(X_j) = \beta_j X_j$

_Bi-level selection_ is the procedure that simultaneously addresses the two questions above


### Spike-and-Slab Priors
Spike-and-slab priors are a family of mixture distributions that deploys a characterizing structure
  $$\beta|\gamma \sim (1-\gamma)f_{spike}(\beta) + \gamma f_{slab}(\beta)$$

  * Latent indicator $\gamma$ follows a Bernoulli distribution with probability $\theta$
  * Spike density $f_{spike}(x)$ concentrates around 0 for small effects
  * Slab density $f_{slab}(x)$ is a flat density for large effects
  
* Natural procedure to select variables via posterior distribution of $\gamma$
* Markov chain Monte Carlo is not compelling for high-dimensional data analysis

### Spike-and-Slab LASSO Priors
* Double exponential distributions as the spike and slab distributions
  $$\beta|\gamma \sim (1-\gamma)DE(0, s_0) + \gamma DE(0, s_1), 0 < s_0 < s_1$$
  * Seamless variable selection as coefficients shrinkage to 0
  * Computation advantages via Expectation-Maximization (EM) algorithms
  
* Group spike-and-slab LASSO
  * Structure underlying predictors, e.g. gene pathways, bases of a spline function
  * Structured prior on $\gamma$
  $$
  \gamma_k | \theta_j ~ Binomial(1, \theta_j), k \in j
  $$

### Problem: High-dimensional Spline Model

How to jointly model signal sparsity and function smoothness, while capable of bi-level selection?

* Excess shrinkage due to ignoring smooth penalty completely
    * Group lasso penalty [@Ravikumar2009; @Huang2010], group SCAD penalty [@Wang2007; @Xue2009]
    * Global penalty VS adaptive penalty

* All-in-all-out selection
    * Can not detect if a function is linear, e.g. spike-and-slab grouped LASSO prior [@Bai2020; @Bai2021]
    * Failed to select function as whole, e.g. group spike-and-slab LASSO prior

* Computational prohibitive algorithms
    * MCMC algorithms doesn't scale well for high-dimensional models [@Scheipl2012]

# Dissertation

### Objectives
* To develop statistical models that improve curve interpolation and outcome prediction
  * Local adaption of sparse penalty and smooth penalty
  * Bi-level selection for linear and nonlinear effect
* To develop a fast and scalable algorithm
* To implement a user-friendly statistical software

### Scope
Scope of this dissertation
* BHAM
* Survival Model
* R package `BHAM`

# References {.allowframebreaks}
