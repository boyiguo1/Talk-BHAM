% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
  aspectratio=169]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usetheme[]{Szeged}
\usecolortheme{spruce}
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Spike-and-Slab Additive Models And Fast Algorithms For High-Dimensional Data Analysis},
  pdfauthor={Boyi Guo},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\newif\ifbibliography
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{bm}
\usepackage{tcolorbox}


\usetikzlibrary{shapes.geometric, arrows, positioning, calc, matrix, backgrounds, fit}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\tp}{*}
\newcommand{\pr}{\text{Pr}}
\newcommand{\repa}{\text{repa}}
\newcommand{\simiid}{\overset{\text{iid}}{\sim}}
\newcommand{\bg}[1]{\textcolor{red}{#1}}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Spike-and-Slab Additive Models And Fast Algorithms For
High-Dimensional Data Analysis}
\author{Boyi Guo}
\date{July 12th, 2022}
\institute{Department of Biostatistics\\
University of Alabama at Birmingham}

\begin{document}
\frame{\titlepage}

\begin{frame}{Dissertation Committee}
\protect\hypertarget{dissertation-committee}{}
\begin{itemize}
\tightlist
\item
  Chair: Nengjun Yi, Ph.D.
\item
  Member (in alphabetical order):

  \begin{itemize}
  \tightlist
  \item
    AKM Fazlur Rahman, Ph.D.
  \item
    Byron C. Jaeger, Ph.D.
  \item
    D. Leann Long, Ph.D.
  \item
    Michael E. Seifert, M.D.
  \end{itemize}
\end{itemize}
\end{frame}

\hypertarget{outline}{%
\section*{Outline}\label{outline}}

\begin{frame}[fragile]{Outline}
\begin{itemize}
\tightlist
\item
  Background

  \begin{itemize}
  \tightlist
  \item
    Spline Model Development
  \item
    Bayesian Regularization
  \item
    Bayesian Variable Selection
  \end{itemize}
\item
  Dissertation

  \begin{itemize}
  \tightlist
  \item
    Bayesian Hierarchical Additive Models
  \item
    Additive Cox Proportional Hazards Model
  \item
    R package \texttt{BHAM}
  \end{itemize}
\item
  Conclusion

  \begin{itemize}
  \tightlist
  \item
    Future Research
  \item
    Questions \& Answers
  \item
    Closing Statement \& Acknowledgment
  \end{itemize}
\end{itemize}
\end{frame}

\hypertarget{background}{%
\section{Background}\label{background}}

\hypertarget{spline-model-development}{%
\subsection{Spline Model Development}\label{spline-model-development}}

\begin{frame}{Spline Model Development}
\begin{quote}
``It is extremely unlikely that the true (effect) function f(X)
(\emph{on the outcome}) is actually linear in X.'' \hspace*{2cm}
\end{quote}

\begin{quote}
--- Hastie, Tibshirani, and Friedman (2009) PP. 139
\end{quote}

\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=Question]
How to model nonlinear effects?
\end{tcolorbox}
\end{frame}

\begin{frame}{Spline Functions}
\protect\hypertarget{spline-functions}{}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
A \emph{spline} function is a piece-wise polynomial function \[
B(x) = \sum\limits_{k = 1}^K \beta_k b_k(x) \equiv  \bs X^T \bs \beta
\] \(b_k(x)\) are the \emph{basis functions}, possibly truncated power
basis and b-spline basis. (Simon N. Wood 2017)
\end{column}

\begin{column}{0.48\textwidth}
\includegraphics{phd_defense_slides_files/figure-beamer/plot1-1.pdf}
\end{column}
\end{columns}

\begin{itemize}
\tightlist
\item
  For simplicity, we assume all functions have \(K\) basis functions and
  knots of functions are equidistance.
\end{itemize}
\end{frame}

\begin{frame}{Generalized Additive Models with Splines}
\protect\hypertarget{generalized-additive-models-with-splines}{}
\textbf{Generalized additive model} (Hastie and Tibshirani 1987) is
expressed \begin{align*}
  y_i &\simiid EF(\mu_i, \phi), \quad i = 1, \dots, n\\
  g(\mu_i) &= \beta_0 + B(x_i) = \beta_0 + \bs X_i^T \bs \beta ,  \quad \mathbb{E}\left[B(X)\right] = 0 
\end{align*} where \(B(x_i)\) is the spline function, \(g(\cdot)\) is a
link function, \(\phi\) is the dispersion parameter

\vspace*{0.2cm}

\begin{itemize}
\tightlist
\item
  Model fitting follows the generalized linear models, e.g.~ordinary
  least square for Gaussian outcome \[
  \boldsymbol{\hat \beta} = \text{arg}\min \sum\limits^n_{i=1} \left[y_i - \beta_0 - \bs X_i^T \bs \beta \right]^2
  \]
\end{itemize}
\end{frame}

\begin{frame}{Problem: Function Smoothness}
\protect\hypertarget{problem-function-smoothness}{}
\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=Question]
How to mathematically define and estimate the smoothness of spline functions?
\end{tcolorbox}

\includegraphics{phd_defense_slides_files/figure-beamer/spline_overfitting-1.pdf}
\end{frame}

\hypertarget{bayesian-regularization}{%
\subsection{Bayesian Regularization}\label{bayesian-regularization}}

\begin{frame}{Smoothing Spline Model}
\protect\hypertarget{smoothing-spline-model}{}
\begin{itemize}
\item
  Smoothing penalty
  \(\lambda \int B^{''}(X)^2dx = \lambda \bs \beta^T \bs S \bs\beta\)

  \begin{itemize}
  \tightlist
  \item
    The smoothing penalty matrix \(\bs S\) is known given \(\bs X\)
  \item
    \(\bs S\) is symmetric and positive semi-definite
  \end{itemize}
\item
  Penalized Least Square for Gaussian Outcome \[
  \boldsymbol{\hat \beta} = \text{arg}\min \sum\limits^n_{i=1} \sum\limits^n_{i=1} \left[y_i - \beta_0 - \bs X_i^T \bs \beta \right]^2 + \lambda \bs \beta^T \bs S \bs\beta
  \]
\item
  The smoothing parameter \(\lambda\) is a tuning parameter, selected
  via cross-validation
\end{itemize}
\end{frame}

\begin{frame}{Problem: Multiple Predictor Model}
\protect\hypertarget{problem-multiple-predictor-model}{}
When a model contains multiple spline functions for variables
\(X_1, \dots, X_p\), the penalized least square estimator is \[
\boldsymbol{\hat \beta} = \text{arg}\min \sum\limits^n_{i=1}  \left[y_i - \beta_0 - \sum\limits_{j=1}^p \bs X_{ij}^T \bs \beta_j \right]^2 + \sum\limits_{j=1}^p\lambda_j \bs \beta_j^T \bs S_j \bs\beta_j
\]

\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=Question]
How to choose $\lambda_i$ for $i = 1, \dots, p$?
  \begin{itemize}
    \item Global smoothing: $\lambda_1 = \cdots =\lambda_p$
    \item Adaptive smoothing: unique $\lambda_i$ for $i = 1, \dots, p$
  \end{itemize}
\end{tcolorbox}
\end{frame}

\begin{frame}{Bayesian Regularization}
\protect\hypertarget{bayesian-regularization-1}{}
\begin{itemize}
\tightlist
\item
  Bayesian regularization is the Bayesian analogy of penalized models by
  using regularizing priors \[
   \text{Bayesian ridge:} \quad \beta \sim N(0, \tau^2) \rightarrow  \lambda = \sigma^2/\tau^2
  \]
\item
  Adaptive shrinkage with hierarchical priors \[
   \tau^2_j \simiid IG(a, b)
  \]
\item
  Adaptive smoothing

  \begin{itemize}
  \tightlist
  \item
    Random walk prior on b-spline bases with IG hyperprior (Lang and
    Brezger 2004)
  \item
    Log-normal spline model for \(\tau^2_k\) (Baladandayuthapani,
    Mallick, and Carroll 2005)
  \end{itemize}
\end{itemize}
\end{frame}

\hypertarget{bayesian-variable-selection}{%
\subsection{Bayesian Variable
Selection}\label{bayesian-variable-selection}}

\begin{frame}{Problem: Functional Selection}
\protect\hypertarget{problem-functional-selection}{}
In the context of variable selection and high-dimensional statistics, we
always assume some variables are not effective or predictive of the
outcome.

\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=Question]
How to statistically detect
  \begin{itemize}
    \item if a variable is predictive to the outcome, $B_j(X_j) = 0$
    \item if a variable has a nonlinear relationship with the outcome, $B_j(X_j) = \beta_j X_j$
  \end{itemize}
\end{tcolorbox}

\emph{Bi-level selection} is the procedure that simultaneously addresses
the two questions above
\end{frame}

\begin{frame}{Spike-and-Slab Priors}
\protect\hypertarget{spike-and-slab-priors}{}
Spike-and-slab priors are a family of mixture distributions that employs
a characterizing structure
\[\beta|\gamma \sim (1-\gamma)f_{spike}(\beta) + \gamma f_{slab}(\beta)\]

\begin{itemize}
\item
  Latent indicator \(\gamma\) follows a Bernoulli distribution with
  probability \(\theta\)
\item
  Slab density \(f_{slab}(x)\) is a flat density for large effects
\item
  Spike density \(f_{spike}(x)\) concentrates around 0 for small effects
\item
  Natural procedure to select variables via posterior distribution of
  \(\gamma\)
\item
  Markov chain Monte Carlo is not compelling for high-dimensional data
  analysis
\end{itemize}
\end{frame}

\begin{frame}{Spike-and-Slab LASSO Priors}
\protect\hypertarget{spike-and-slab-lasso-priors}{}
\begin{itemize}
\tightlist
\item
  Double exponential distributions as the spike and slab distributions
  \[\beta|\gamma \sim (1-\gamma)DE(0, s_0) + \gamma DE(0, s_1), 0 < s_0 < s_1\]

  \begin{itemize}
  \tightlist
  \item
    Computation advantages via Expectation-Maximization (EM) algorithms
  \item
    Seamless variable selection as coefficients shrink to 0
  \end{itemize}
\item
  Group spike-and-slab LASSO prior

  \begin{itemize}
  \tightlist
  \item
    Structure among predictors, e.g.~gene pathways, bases of a spline
    function
  \item
    Structured prior
    \(\gamma_k | \theta_j \simiid Binomial(1, \theta_j), k \in j\)
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Problem: High-dimensional Spline Model}
\protect\hypertarget{problem-high-dimensional-spline-model}{}
\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=Question]
How to jointly model signal sparsity and function smoothness, while capable of bi-level selection?
\end{tcolorbox}

\begin{itemize}
\tightlist
\item
  Excess shrinkage due to negligence of smooth penalty

  \begin{itemize}
  \tightlist
  \item
    Group LASSO penalty (Ravikumar et al. 2009; Huang, Horowitz, and Wei
    2010), group SCAD penalty (Wang, Chen, and Li 2007; Xue 2009)
  \end{itemize}
\item
  All-in-all-out selection

  \begin{itemize}
  \tightlist
  \item
    Failed to select function as a whole, e.g.~group spike-and-slab
    LASSO prior
  \item
    Can not detect if a function is linear, e.g.~spike-and-slab grouped
    LASSO prior (Bai et al. 2020; Bai 2021)
  \end{itemize}
\end{itemize}
\end{frame}

\hypertarget{dissertation}{%
\section{Dissertation}\label{dissertation}}

\begin{frame}{Objectives}
\protect\hypertarget{objectives}{}
\begin{itemize}
\tightlist
\item
  To develop statistical models that improve curve interpolation and
  outcome prediction

  \begin{itemize}
  \tightlist
  \item
    Adaptive regularization that accounts for signal sparsity and
    function smoothness
  \item
    Bi-level selection for linear and nonlinear effect
  \end{itemize}
\item
  To develop a fast and scalable algorithm
\item
  To implement a user-friendly statistical software
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Projects}
\protect\hypertarget{projects}{}
\begin{itemize}
\item
  \textbf{Guo, B.}, Jaeger, B. C., Rahman, A. F., Long, D. L., Yi, N.
  (2022). Spike-and-Slab LASSO generalized additive models and scalable
  algorithms for high-dimensional data analysis. \emph{Statistics in
  Medicine}. doi: \url{https://doi.org/10.1002/sim.9483}
\item
  \textbf{Guo, B.}, Jaeger, B. C., Rahman, A. F., Long, D. L., Yi, N.
  (2022). A scalable and flexible Cox proportional hazards model for
  high-dimensional survival prediction and functional selection.
  \emph{arXiv}. doi: \url{https://doi.org/10.48550/arXiv.2205.11600}
\item
  \textbf{Guo, B.}, Yi, N. (2022). \texttt{BHAM}: An R Package to Fit
  Bayesian Hierarchical Additive Models for High-dimensional Data
  Analysis. \emph{arXiv}. doi:
  \url{ https://doi.org/10.48550/arXiv.2207.02348}
\end{itemize}
\end{frame}

\hypertarget{bayesian-hierarchical-additive-models}{%
\subsection{Bayesian Hierarchical Additive
Models}\label{bayesian-hierarchical-additive-models}}

\begin{frame}{Generalized Additive Model}
\protect\hypertarget{generalized-additive-model}{}
Given the data \(\{y_i, x_{i1}, \dots ,x_{ip}\}_{i=1}^n\) where
\(p >> n\) \begin{align*}
y_i &\overset{\text{i.i.d.}}{\sim} EF(\mu_i, \phi), \quad i = 1, \dots, n.\\
g(\mu_i) &= \beta_0 + \sum\limits^p_{j=1}B_j(x_{ij}) = \beta_0 + \sum\limits^p_{j=1}\sum\limits_{k=1}^K \beta_{jk}b_{jk}(x_{ij}) = \beta_0 + \sum\limits^p_{j=1}\bs X_{ij}^T \bs \beta_j
\end{align*}

\begin{itemize}
\tightlist
\item
  Each spline function consists of \(K\) bases
\item
  Identifiability constraint:
  \(\mathbb{E}\left[B_j(X)\right] = 0, j=1,\dots, p\)
\end{itemize}
\end{frame}

\begin{frame}{Spline Function Reparameterization}
\protect\hypertarget{spline-function-reparameterization}{}
\begin{itemize}
\item
  Smoothing penalty \(\lambda \bs \beta^T \bs S \bs \beta\)

  \begin{itemize}
  \tightlist
  \item
    \(\bs S\) is symmetric and positive semi-definite
  \item
    \(\bs S = \bm U \bm D \bm U^T\) via eigendecompostion
  \end{itemize}
\item
  Isolate the linear and nonlinear components
  \[\bm X^T \bm \beta = (\bm X^T \bm U) (\bm U^T \bm \beta)= X^{0} \beta + \bs X^\tp \bs \beta^\tp\]
\item
  Benefits

  \begin{itemize}
  \tightlist
  \item
    Motivate bi-level selection
  \item
    Implicit modeling of function smoothness
  \item
    Reduce computation load with conditionally independent prior of
    basis coefficients
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Two-part Spike-and-slab LASSO (SSL) Prior}
\protect\hypertarget{two-part-spike-and-slab-lasso-ssl-prior}{}
\begin{itemize}
\item
  SSL prior for the linear coefficient and modified group SSL priors for
  nonlinear coefficients \begin{align*}
  \beta_{j} |\gamma_{j},s_0,s_1 &\sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1) \\
  \beta^\tp_{jk} | \gamma^\tp_{j},s_0,s_1 &\simiid DE(0,(1-\gamma^\tp_{j}) s_0 + \gamma^\tp_{j} s_1), k = 1, \dots, K-1
  \end{align*}

  \begin{itemize}
  \tightlist
  \item
    \(\gamma_{j}\) controls the inclusion of linear component
  \item
    \(\gamma_{j}^\tp\) controls the inclusion of nonlinear component
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Effect Hierarchy}
\protect\hypertarget{effect-hierarchy}{}
\begin{itemize}
\item
  \emph{Effect hierarchy} assumes lower-order effects are more likely to
  be active than higher-order effects
\item
  Structured prior on latent indicators \(\gamma_j\) and
  \(\gamma^\tp_{j}\) \[
  \gamma_{j} | \theta_j \sim Bin(\gamma_{j}|1, \theta_j),\quad
  \gamma_{j}^\tp | \gamma_{j}, \theta_j \sim Bin(1, \gamma_{j}\theta_j),
  \]

  \begin{itemize}
  \tightlist
  \item
    Simplification via analytic integration \[
    \gamma_{j}^\tp | \theta_j \sim Bin(1, \theta_j^2),
    \]
  \end{itemize}
\item
  Adaptive shrinkage \[
  \theta_j \sim \text{Beta}(a,b)
  \]
\end{itemize}
\end{frame}

\begin{frame}{Visual Representation}
\protect\hypertarget{visual-representation}{}
\begin{figure}
\centering
\resizebox{12cm}{5.5cm}{
\begin{tikzpicture} [
staticCompo/.style = {rectangle, minimum width=1cm, minimum height=1cm,text centered, draw=black, fill=blue!30},
outCome/.style={ellipse, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=blue!30},
mymatrix/.style={matrix of nodes, nodes=outCome, row sep=1em},
PriorBoarder/.style={rectangle, minimum width=5cm, minimum height=10cm, text centered, fill=lightgray!30},
background/.style={rectangle, fill=gray!10,inner sep=0.2cm, rounded corners=5mm}
]

\matrix (linearPrior) [matrix of nodes, column sep = 0mm, row sep = 0.7cm] {
  \node (linearGamma) [outCome] { $\gamma_j \sim Bin(1, \theta_j) $ };\\
  \node (linearBeta) [outCome] { $\beta_j \sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1)$};\\
};
\matrix (penPrior) [right = 2cm of linearPrior, matrix of nodes, column sep = 0mm, row sep = 0.7cm] {
  \node (penGamma) [outCome] { $\gamma_{j}^\tp \sim Bin(1, \gamma_{j}\theta_j)$ };\\
  \node (penBeta) [outCome] { $\beta_{jk}^\tp \sim  DE(0,(1-\gamma^\tp_{j}) s_0 + \gamma^\tp_{j} s_1)$};\\
};


\node (s) [staticCompo]  at ($(linearBeta)!0.5!(penBeta)$)  {($s_0, s_1$)};
\node (Beta) [staticCompo, below = 1cm of s] {$\bs \beta = (\beta_1, \bs \beta^\tp_1, \dots,\beta_j, \bs \beta^\tp_j , \dots,\beta_p, \bs \beta^\tp_p) $};
\node (Theta)[outCome, above = 2cm of s] {$\theta_{j} \sim Beta(a, b)$};
\node (ab)[staticCompo, above = 0.5cm of Theta] {$(a, b)$};
\node (Y) [outCome, below = 1cm of Beta] {$y_i \sim Expo. Fam. (g^{-1}(\bs \beta^T \bs X_i), \phi)$};

\draw[->] (Theta) -- (linearGamma);
\draw[->] (Theta) -- (penGamma);
\draw[->] (linearGamma) -- (linearBeta) ;
\draw[->] (penGamma) -- (penBeta);
\draw[->] (linearGamma) -- (penGamma);
\draw[->] (ab) -- (Theta);
\draw[->] (s) -- (linearBeta) ;
\draw[->] (s) -- (penBeta);
\draw[->] (linearBeta) -- (Beta);
\draw[->] (penBeta) -- (Beta);
\draw[->] (Beta) --  (Y);


\begin{pgfonlayer}{background}
  \node [background,
   fit=(linearGamma) (linearBeta),
   label=above:Linear Space:] {};
  \node [background,
    fit=(penGamma) (penBeta),
    label=above:Nonlinear Space:] {};
\end{pgfonlayer}

\end{tikzpicture}
}
\end{figure}
\end{frame}

\begin{frame}{EM-Cooridante Descent Algrithm for Scalable Model Fitting}
\protect\hypertarget{em-cooridante-descent-algrithm-for-scalable-model-fitting}{}
We are interested in estimating
\(\Theta = \{\bm \beta, \bm \theta, \phi\}\) using optimization based
algorithm for scalability purpose

\begin{itemize}
\tightlist
\item
  Basic Ideas

  \begin{itemize}
  \tightlist
  \item
    Treat \(\gamma\)s as the ``missing data'' in the EM procedure
  \item
    Quantify the expectation of log posterior density function of
    \(\Theta\) with respect to \(\gamma\) conditioning on
    \(\Theta^{(t-1)}\)
  \item
    Maximize two parts of the objective function independently
  \end{itemize}
\item
  Previous applications in high-dimensional data analysis

  \begin{itemize}
  \tightlist
  \item
    EMVS (Ročková and George 2014), Spike-and-slab LASSO (Ročková and
    George 2018)
  \item
    BhGLM (Yi et al. 2019)
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Decomposition of Objective Function}
\protect\hypertarget{decomposition-of-objective-function}{}
We aim to maximize the log posterior density of \(\Theta\) by averaging
over all possible values of \(\bm \gamma\)
\[ \log f(\Theta, \bs \gamma| \textbf{y}, \textbf{X}) = Q_1(\bs \beta, \phi) + Q_2 (\bs \gamma,\bs \theta),\]

\begin{itemize}
\item
  L\(_1\)-penalized likelihood function of \(\bs \beta, \phi\)
  \[Q_1 \equiv Q_1(\bs \beta, \phi) = \log f(\textbf{y}|\bs \beta, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|\gamma_j)+\sum\limits_{k=1}^{K_j} \log f(\beta^{\tp}_{jk}|\gamma^{\tp}_{j})\right]\]
\item
  Posterior density of \(\theta\) given data points \(\gamma\)s
  \[Q_2 \equiv Q_2(\bs\gamma,\bs\theta) = \sum\limits_{j=1}^{p} \left[ (\gamma_j+\gamma_{j}^{\tp})\log \theta_j + (2-\gamma_j-\gamma_{j}^{\tp}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log f(\theta_j).\]
\item
  \(Q_1\) and \(Q_2\) are independent conditioning on \(\gamma\)s
\end{itemize}
\end{frame}

\begin{frame}{Summary of EM-Coordinate Descent Algorithm}
\protect\hypertarget{summary-of-em-coordinate-descent-algorithm}{}
\begin{itemize}
\tightlist
\item
  E-step

  \begin{itemize}
  \tightlist
  \item
    Formulate
    \(E_{\bm \gamma|\Theta^{(t)}}\left[Q(\Theta, \bm \gamma)\right] = E(Q_1) + E(Q_2)\)

    \begin{itemize}
    \tightlist
    \item
      \(E(Q_1)\) is a penalized likelihood function of \(\beta, \phi\)
    \item
      \(E(Q_2)\) is a posterior density of \(\theta\) given
      \(E(\gamma)\)
    \item
      \(E(Q_1)\) and \(E(Q_2)\) are conditionally independent
    \end{itemize}
  \item
    Calculate \(E(\gamma_{j})\), \(E(\gamma^\tp_{j})\) and the penalties
    parameters by Bayes' theorem
  \end{itemize}
\item
  M-step:

  \begin{itemize}
  \tightlist
  \item
    Use Coordinate Descent to fit the penalized model in \(E(Q_1)\) to
    update \(\beta, \phi\)
  \item
    Closed form calculation via \(E(Q_2)\) to update \(\theta\)
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Tuning Parameter Selection}
\protect\hypertarget{tuning-parameter-selection}{}
\begin{itemize}
\tightlist
\item
  \(s_0\) and \(s_1\) are tuning parameters
\item
  Empirically, \(s_1\) has extremely small effect on changing the
  estimates
\item
  Focus on tuning \(s_0\)
\item
  Consider a sequence of \(L\) ordered values
  \(\{s_0^l\}: 0 < s_0^1 < s_0^2 < \dots < s_0^L < s_1\)
\item
  Cross-validation to choose optimal value for \(s_0\)
\end{itemize}
\end{frame}

\begin{frame}{Simulation Study}
\protect\hypertarget{simulation-study}{}
\begin{itemize}
\item
  Follow the data generating process introduced in Bai et al. (2020).
\item
  \(n_{train} = 500\), \(n_{test}=1000\)
\item
  \(p=4, 10, 50, 200\) \[
  g(\mu) = 5 \sin(2\pi X_1) - 4 \cos(2\pi X_2 -0.5) + 6(X_3-0.5) - 5(X_4^2 -0.3),
  \]
\item
  \(f_j(x_j) = 0\) for \(j = 5, \dots, p\).
\item
  2 types of outcome: Gaussian (\(\phi=1\)), Binomial
\item
  Splines are constructed using 10 knots
\item
  50 Iterations
\end{itemize}
\end{frame}

\begin{frame}{Comparison \& Metircs}
\protect\hypertarget{comparison-metircs}{}
\begin{itemize}
\tightlist
\item
  Methods of comparison

  \begin{itemize}
  \tightlist
  \item
    Proposed model BHAM
  \item
    Linear LASSO model as the benchmark
  \item
    mgcv (S. N. Wood 2004)
  \item
    COSSO (Zhang and Lin 2006) and adaptive COSSO(Storlie et al. 2011)
  \item
    Sparse Bayesian GAM (Bai 2021)
  \item
    spikeSlabGAM (Scheipl, Fahrmeir, and Kneib 2012)
  \end{itemize}
\item
  Metrics

  \begin{itemize}
  \tightlist
  \item
    Prediction: \(R^2\) for continuous outcomes, AUC for binary outcomes
  \item
    Variable Selection: positive predictive value (precision), true
    positive rate (recall), and Matthews correlation coefficient (MCC)
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Prediction Performance}
\protect\hypertarget{prediction-performance}{}
\begin{itemize}
\tightlist
\item
  Linear LASSO Model performs bad and mgcv performs well
\item
  BHAM performs better than COSSO, adaptive COSSO and spikeSlabGAM
\item
  BHAM performs better than SB-GAM in low-dimensional case but slightly
  worse in the high-dimensional setting
\item
  BHAM is much faster than SB-GAM in fitting models
\end{itemize}
\end{frame}

\begin{frame}{Variable Selection Performance}
\protect\hypertarget{variable-selection-performance}{}
\begin{itemize}
\tightlist
\item
  SB-GAM has the best variable selection performance
\item
  BHAM has conservative selection
\item
  BHAM and spikeSlabGAM have trade-offs for bi-level selection

  \begin{itemize}
  \tightlist
  \item
    spikeSlabGAM tends to select only the nonlinear component of the
    function
  \item
    BHAM is more likely to select both parts
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Metabolites Data Applications}
\protect\hypertarget{metabolites-data-applications}{}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{itemize}
\tightlist
\item
  Emory Cardiovascular Biobank

  \begin{itemize}
  \tightlist
  \item
    Three-year all-cause mortality
  \item
    p = 200 and N = 454
  \item
    5-knot cubic spline
  \end{itemize}
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\begin{itemize}
\tightlist
\item
  Weight Loss Maintenance Cohort

  \begin{itemize}
  \tightlist
  \item
    Standardized percent change in insulin resistance
  \item
    p = 483 and N = 237
  \item
    5-knot cubic spline
  \end{itemize}
\end{itemize}
\end{column}
\end{columns}

\begin{itemize}
\tightlist
\item
  Compared to SB-GAM, BHAM has better prediction performance and
  substantial computation advantage
\end{itemize}
\end{frame}

\begin{frame}{Emory Cardiovascular Biobank}
\protect\hypertarget{emory-cardiovascular-biobank}{}
\begin{figure}
\centering
\includegraphics[height=6cm]{ECB_plot_wide.pdf}
\end{figure}
\end{frame}

\hypertarget{additive-cox-proportional-hazards-model}{%
\subsection{Additive Cox Proportional Hazards
Model}\label{additive-cox-proportional-hazards-model}}

\begin{frame}{Model \& Objective Functions}
\protect\hypertarget{model-objective-functions}{}
\begin{itemize}
\tightlist
\item
  Cox proportional hazard model with event time \(t_i\) \[
  h(t_i) = h_0(t_i)\exp(\sum\limits^p_{j=1}B_j(x_{ij})) , \quad i = 1, \dots, n.
  \]

  \begin{itemize}
  \tightlist
  \item
    No intercept term because of the baseline hazard function
  \end{itemize}
\item
  Model fitting

  \begin{itemize}
  \tightlist
  \item
    Replace likelihood function with partial likelihood function \[
    \hat h_0(t_i|\beta) = d_i/\sum\limits_{i^\prime \in R(t_i)} exp(X_{i^\prime}\beta).
    \]
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Two-part Spike-and-slab LASSO (SSL) Prior}
\protect\hypertarget{two-part-spike-and-slab-lasso-ssl-prior-1}{}
\begin{itemize}
\tightlist
\item
  SSL prior for the linear coefficient and group SSL priors for
  nonlinear coefficients \begin{align*}
  \beta_{j} |\gamma_{j},s_0,s_1 &\sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1) \\
  \beta^\tp_{jk} | \gamma^\tp_{j},s_0,s_1 &\simiid DE(0,(1-\gamma^\tp_{j}) s_0 + \gamma^\tp_{j} s_1), k = 1, \dots, K_j
  \end{align*}
\item
  Effect hierarchy enforced latent inclusion indicators \(\gamma_j\) and
  \(\gamma^\tp_{j}\) for bi-level selection \[
  \gamma_{j} | \theta_j \sim Bin(\gamma_{j}|1, \theta_j),\quad
  \gamma_{j}^\tp | \gamma_{j}, \theta_j \sim Bin(1, \gamma_{j}\theta_j),
  \]
\item
  Local adaptivity of signal sparsity and function smoothness \[
  \theta_j \sim \text{Beta}(a,b)
  \]
\end{itemize}
\end{frame}

\begin{frame}{Summary of EM-Coordinate Descent Algorithm}
\protect\hypertarget{summary-of-em-coordinate-descent-algorithm-1}{}
\begin{itemize}
\tightlist
\item
  E-step

  \begin{itemize}
  \tightlist
  \item
    Formulate
    \(E_{\bm \gamma|\Theta^{(t)}}\left[Q(\Theta, \bm \gamma)\right] = E(Q_1) + E(Q_2)\)

    \begin{itemize}
    \tightlist
    \item
      \(E(Q_1)\) is a penalized likelihood function of \(\beta, \phi\)
    \item
      \(E(Q_2)\) is a posterior density of \(\theta\) given
      \(E(\gamma)\)
    \item
      \(E(Q_1)\) and \(E(Q_2)\) are conditionally independent
    \end{itemize}
  \item
    Calculate \(E(\gamma_{j})\), \(E(\gamma^\tp_{j})\) and the penalties
    parameters by Bayes' theorem
  \end{itemize}
\item
  M-step:

  \begin{itemize}
  \tightlist
  \item
    Use Coordinate Descent to fit the penalized model in \(E(Q_1)\) to
    update \(\beta, \phi\)
  \item
    Closed form calculation via \(E(Q_2)\) to update \(\theta\)
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Simulation Study}
\protect\hypertarget{simulation-study-1}{}
\begin{itemize}
\item
  Follow the data generating process introduced in Bai et al. (2020).
\item
  \(n_{train} = 500\), \(n_{test}=1000\)
\item
  \(p=4, 10, 50, 100, 200\)
\item
  Survival and censoring time follow Weibull distribution \[
  \log \eta = (x_1 + 1)^2/5 + \exp (x_2 + 1)/25 + 3\text{sin} (x_3)/2 + (1.4x_4 + 0.5)/2
  \]
\item
  Censoring rate is controlled at \{0.15, 0.3, 0.45\}
\item
  Splines are constructed using 10 knots
\item
  50 Iterations
\end{itemize}
\end{frame}

\begin{frame}{Comparison \& Metircs}
\protect\hypertarget{comparison-metircs-1}{}
\begin{itemize}
\tightlist
\item
  Methods of comparison

  \begin{itemize}
  \tightlist
  \item
    Proposed model BHAM
  \item
    Linear LASSO model as the benchmark
  \item
    mgcv (S. N. Wood 2004)
  \item
    COSSO (Zhang and Lin 2006) and adaptive COSSO(Storlie et al. 2011)
  \end{itemize}
\item
  Metrics

  \begin{itemize}
  \tightlist
  \item
    Out-of-sample deviance \& Concordance
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Prediction Performance}
\protect\hypertarget{prediction-performance-1}{}
\begin{itemize}
\tightlist
\item
  Linear LASSO Model performs bad in general
\item
  Low dimensional settings:

  \begin{itemize}
  \tightlist
  \item
    mgcv performs the best
  \item
    BHAM performs as good as mgcv
  \end{itemize}
\item
  High dimensional setting:

  \begin{itemize}
  \tightlist
  \item
    BHAM performs better than COSSO models as p increases and more
    censoring events
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Emory Cardiovascular Biobank}
\protect\hypertarget{emory-cardiovascular-biobank-1}{}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{itemize}
\tightlist
\item
  All-cause mortality among patents undergoing cardiac catheterization
\item
  Sample size N=454 and number of features p=200
\item
  5-knot cubic spline
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\includegraphics[width=\textwidth,height=0.6\textheight]{ECB_bcam_KM.pdf}
\end{column}
\end{columns}
\end{frame}

\hypertarget{r-package-bham}{%
\subsection{\texorpdfstring{R Package
\texttt{BHAM}}{R Package BHAM}}\label{r-package-bham}}

\begin{frame}{R Package \texttt{BHAM}}
\begin{itemize}
\tightlist
\item
  Model formulation for high-dimensional data
\item
  Model fitting and tuning
\item
  Model summary and variable selection
\item
  Spline function visualization
\item
  Website via
  \href{https://boyiguo1.github.io/BHAM/}{\emph{boyiguo1.github.io/BHAM}}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Design Matrix of Spline Fucntions}
\protect\hypertarget{design-matrix-of-spline-fucntions}{}
\begin{itemize}
\tightlist
\item
  Flexible spline function formulation for high-dimensional data
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spline\_df }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{tribble}\NormalTok{(}
    \SpecialCharTok{\textasciitilde{}}\NormalTok{Var, }\SpecialCharTok{\textasciitilde{}}\NormalTok{Func, }\SpecialCharTok{\textasciitilde{}}\NormalTok{Args,}
    \StringTok{"X1"}\NormalTok{,  }\StringTok{"s"}\NormalTok{, }\StringTok{"bs=\textquotesingle{}cr\textquotesingle{}, k=5"}\NormalTok{,}
    \StringTok{"X2"}\NormalTok{,  }\StringTok{"s"}\NormalTok{, }\ConstantTok{NA}\NormalTok{,}
    \StringTok{"X3"}\NormalTok{,  }\StringTok{"s"}\NormalTok{, }\StringTok{""}\NormalTok{)}
\NormalTok{spline\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
    \AttributeTok{Var =} \FunctionTok{setdiff}\NormalTok{(}\FunctionTok{names}\NormalTok{(dat), }\StringTok{"y"}\NormalTok{),}
    \AttributeTok{Func =} \StringTok{"s"}\NormalTok{,}
    \AttributeTok{Args =}\StringTok{"bs=\textquotesingle{}cr\textquotesingle{}, k=7"}\NormalTok{)}
\NormalTok{train\_sm\_dat }\OtherTok{\textless{}{-}}\NormalTok{ BHAM}\SpecialCharTok{::}\FunctionTok{construct\_smooth\_data}\NormalTok{(spline\_df, dat)}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}[fragile]{Model Fitting Functions}
\protect\hypertarget{model-fitting-functions}{}
\begin{itemize}
\tightlist
\item
  Model fitting via \texttt{bamlasso}

  \begin{itemize}
  \tightlist
  \item
    Argument \texttt{family} for generalized and survival outcomes
  \item
    Argument \texttt{ss} for spike-and-slab LASSO scale parameters
  \item
    Argument \texttt{group} for group structures among predictors
  \end{itemize}
\item
  Model tuning via \texttt{tune}

  \begin{itemize}
  \tightlist
  \item
    Argument \texttt{nfolds}, \texttt{ncv} for nested cross-validation
  \item
    Argument \texttt{s0} for tuning candidates
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Post Fitting Functions}
\protect\hypertarget{post-fitting-functions}{}
\begin{itemize}
\tightlist
\item
  Bi-level selection via \texttt{bamlasso\_var\_selection}
\item
  Make prediction data for splines \texttt{make\_predict\_dat}
\item
  Plot spline functions via \texttt{plot\_smooth\_term}
\end{itemize}
\end{frame}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\hypertarget{future-research}{%
\subsection{Future Research}\label{future-research}}

\begin{frame}{Varying Coefficient Models}
\protect\hypertarget{varying-coefficient-models}{}
\begin{itemize}
\tightlist
\item
  Assume the coefficient of a variable \(X_j\) is a function of a
  covariate \(Z_j\)

  \begin{itemize}
  \tightlist
  \item
    linear model: \(\beta(Z_j) = \beta\)
  \item
    VC model: \(\beta(Z_j) = B(Z_j)\)
  \end{itemize}
\item
  Replace each spline function \(B(z_{ij})\) with
  \(B(z_{ij})x_{ij}\equiv \bs (x_{ij}\bs Z_{ij}^T)\bs \beta_j\)
\item
  Model fitting with EM-Coordinate Descent
\item
  Nonlinear interaction of a continuous variable and a categorical
  variable
\end{itemize}

\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=Question]
  How to model nonlinear interaction of two continuous variables?
\end{tcolorbox}
\end{frame}

\begin{frame}{Smooth Surface Fitting}
\protect\hypertarget{smooth-surface-fitting}{}
\begin{itemize}
\tightlist
\item
  Tensor product of spline functions \[
  B_{js}(x_{ij}, x_{is}) = \sum\limits^K_{P\rho=1} \sum\limits^K_{v=1} \beta_{jspv} b_{j\rho}(x_{ij})b_{sv}(x_{is})
  \]
\item
  Smooth Surface \[
    B_j(x_{ij}) + B_s(x_{is}) + B_{js}(x_{ij}, x_{is}),
  \]
\end{itemize}

\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=Question]
  Can we have a generalized model that accounts fixed effects, nonlinear curves, smooth surfaces, and random effects?
\end{tcolorbox}
\end{frame}

\begin{frame}{Structural Additive Model}
\protect\hypertarget{structural-additive-model}{}
High-dimensional structural additive model can be formulated as \[
g(\mathbb{E}(y_i)) = \bs x_i^T \bs \theta + \bs u_i^T \bs \gamma + B(z_{i1}) + B(z_{i2}, z_{i3}) + B_{spat}(s_i)
\]

\begin{itemize}
\tightlist
\item
  Un-regularized predictors \(\bs x_i\)
\item
  Regularized predictors \(\bs u_i\)
\item
  Predictors with nonlinear effects \(\bs z_i\)
\item
  Spatial random effects with coordinates \(s_i\)
\end{itemize}

Spike-and-slab LASSO prior motivates a seamless process of
variable/functional selection and a scalable optimization-based model
fitting algorithm
\end{frame}

\hypertarget{conclusion-1}{%
\subsection{Conclusion}\label{conclusion-1}}

\begin{frame}[fragile]{Conclusion}
\begin{itemize}
\tightlist
\item
  Identify challenges in high-dimensional GAM with spline functions

  \begin{itemize}
  \tightlist
  \item
    Balance between signal sparsity and function smoothness
  \item
    Bi-level selection to automatically detect linear and nonlinear
    effects
  \end{itemize}
\item
  Statistical contribution

  \begin{itemize}
  \tightlist
  \item
    Two-part spike-and-slab LASSO prior for spline functions
  \item
    Scalable EM-Coordinate Descent algorithms for generalized and
    survival outcomes
  \item
    R package \texttt{BHAM}
  \end{itemize}
\item
  Future Research

  \begin{itemize}
  \tightlist
  \item
    Extension of spike-and-slab LASSO prior in structured additive model
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Acknowledgement}
\protect\hypertarget{acknowledgement}{}
I would like to thank

\begin{columns}[T]
\begin{column}{0.48\textwidth}
\begin{itemize}
\tightlist
\item
  My parents and girlfriend
\item
  Dissertation Committee
\item
  Biostatistics Department

  \begin{itemize}
  \tightlist
  \item
    Faculty
  \item
    Staff
  \item
    Students
  \end{itemize}
\end{itemize}
\end{column}

\begin{column}{0.48\textwidth}
\begin{itemize}
\tightlist
\item
  REGARDS Research Group

  \begin{itemize}
  \tightlist
  \item
    PIs
  \item
    Analytic Team
  \item
    Collaborators
  \end{itemize}
\item
  School of Public Health

  \begin{itemize}
  \tightlist
  \item
    Friends
  \end{itemize}
\item
  Public Health Student Association
\item
  Graduate Student Government
\end{itemize}
\end{column}
\end{columns}
\end{frame}

\hypertarget{advocacy}{%
\subsection{Advocacy}\label{advocacy}}

\begin{frame}{Advocacy}
\begin{figure}
\centering
\includegraphics[width=10cm]{Mentors_VS_Policies.jpg}
\end{figure}
\end{frame}

\begin{frame}{Advocacy}
\protect\hypertarget{advocacy-1}{}
\begin{figure}
\centering
\includegraphics[width=10cm]{all_helps.jpg}
\end{figure}
\end{frame}

\hypertarget{q-a}{%
\subsection{Q \& A}\label{q-a}}

\begin{frame}{Q \& A}
\begin{quote}
TODO (Audience): Ask questions here
\end{quote}
\end{frame}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\begin{frame}[allowframebreaks]{References}
\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Bai2021}{}}%
Bai, Ray. 2021. {``Spike-and-Slab Group Lasso for Consistent Estimation
and Variable Selection in Non-Gaussian Generalized Additive Models.''}
\emph{arXiv:2007.07021v5.}

\leavevmode\vadjust pre{\hypertarget{ref-Bai2020}{}}%
Bai, Ray, Gemma E Moran, Joseph L Antonelli, Yong Chen, and Mary R
Boland. 2020. {``Spike-and-Slab Group Lassos for Grouped Regression and
Sparse Generalized Additive Models.''} \emph{Journal of the American
Statistical Association}, 1--14.

\leavevmode\vadjust pre{\hypertarget{ref-Baladandayuthapani2005}{}}%
Baladandayuthapani, Veerabhadran, Bani K. Mallick, and Raymond J.
Carroll. 2005. {``Spatially Adaptive Bayesian Penalized Regression
Splines (p-Splines).''} \emph{Journal of Computational and Graphical
Statistics} 14 (2): 378--94.

\leavevmode\vadjust pre{\hypertarget{ref-Hastie1987}{}}%
Hastie, Trevor, and Robert Tibshirani. 1987. {``{Generalized additive
models: Some applications}.''} \emph{Journal of the American Statistical
Association} 82 (398): 371--86.
\url{https://doi.org/10.1080/01621459.1987.10478440}.

\leavevmode\vadjust pre{\hypertarget{ref-hastie2009elements}{}}%
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. \emph{The
Elements of Statistical Learning: Data Mining, Inference, and
Prediction}. Springer Science \& Business Media.

\leavevmode\vadjust pre{\hypertarget{ref-Huang2010}{}}%
Huang, Jian, Joel L Horowitz, and Fengrong Wei. 2010. {``Variable
Selection in Nonparametric Additive Models.''} \emph{Annals of
Statistics} 38 (4): 2282.

\leavevmode\vadjust pre{\hypertarget{ref-Lang2004}{}}%
Lang, Stefan, and Andreas Brezger. 2004. {``Bayesian p-Splines.''}
\emph{Journal of Computational and Graphical Statistics} 13 (1):
183--212. \url{https://doi.org/10.1198/1061860043010}.

\leavevmode\vadjust pre{\hypertarget{ref-Ravikumar2009}{}}%
Ravikumar, Pradeep, John Lafferty, Han Liu, and Larry Wasserman. 2009.
{``{Sparse additive models}.''} \emph{Journal of the Royal Statistical
Society: Series B (Statistical Methodology)} 71 (5): 1009--30.
\url{https://doi.org/10.1111/j.1467-9868.2009.00718.x}.

\leavevmode\vadjust pre{\hypertarget{ref-Rockova2014a}{}}%
Ročková, Veronika, and Edward I. George. 2014. {``{EMVS: The EM approach
to Bayesian variable selection}.''} \emph{Journal of the American
Statistical Association} 109 (506): 828--46.
\url{https://doi.org/10.1080/01621459.2013.869223}.

\leavevmode\vadjust pre{\hypertarget{ref-Rockova2018}{}}%
---------. 2018. {``{The Spike-and-Slab LASSO}.''} \emph{Journal of the
American Statistical Association} 113 (521): 431--44.
\url{https://doi.org/10.1080/01621459.2016.1260469}.

\leavevmode\vadjust pre{\hypertarget{ref-Scheipl2012}{}}%
Scheipl, Fabian, Ludwig Fahrmeir, and Thomas Kneib. 2012.
{``{Spike-and-slab priors for function selection in structured additive
regression models}.''} \emph{Journal of the American Statistical
Association} 107 (500): 1518--32.
\url{https://doi.org/10.1080/01621459.2012.737742}.

\leavevmode\vadjust pre{\hypertarget{ref-Storlie2011}{}}%
Storlie, Curtis B, Howard D Bondell, Brian J Reich, and Hao Helen Zhang.
2011. {``Surface Estimation, Variable Selection, and the Nonparametric
Oracle Property.''} \emph{Statistica Sinica} 21 (2): 679.

\leavevmode\vadjust pre{\hypertarget{ref-Wang2007}{}}%
Wang, Lifeng, Guang Chen, and Hongzhe Li. 2007. {``Group SCAD Regression
Analysis for Microarray Time Course Gene Expression Data.''}
\emph{Bioinformatics} 23 (12): 1486--94.

\leavevmode\vadjust pre{\hypertarget{ref-Wood2004}{}}%
Wood, S. N. 2004. {``Stable and Efficient Multiple Smoothing Parameter
Estimation for Generalized Additive Models.''} \emph{Journal of the
American Statistical Association} 99 (467): 673--86.

\leavevmode\vadjust pre{\hypertarget{ref-Wood2017}{}}%
Wood, Simon N. 2017. \emph{{Generalized additive models: An introduction
with R, second edition}}. \url{https://doi.org/10.1201/9781315370279}.

\leavevmode\vadjust pre{\hypertarget{ref-Xue2009}{}}%
Xue, Lan. 2009. {``Consistent Variable Selection in Additive Models.''}
\emph{Statistica Sinica}, 1281--96.

\leavevmode\vadjust pre{\hypertarget{ref-Yi2019}{}}%
Yi, Nengjun, Zaixiang Tang, Xinyan Zhang, and Boyi Guo. 2019. {``BhGLM:
Bayesian Hierarchical GLMs and Survival Models, with Applications to
Genomics and Epidemiology.''} \emph{Bioinformatics} 35 (8): 1419--21.

\leavevmode\vadjust pre{\hypertarget{ref-Zhang2006GAM}{}}%
Zhang, Hao Helen, and Yi Lin. 2006. {``Component Selection and Smoothing
for Nonparametric Regression in Exponential Families.''}
\emph{Statistica Sinica}, 1021--41.

\end{CSLReferences}
\end{frame}

\end{document}
