
@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year={2009},
  publisher={Springer Science \& Business Media}
}

@article{Hastie1987,
abstract = {Generalized additive models have the form $\eta$(x) = $\alpha$ + $\sigma$ fj(xj), where $\eta$ might be the regression function in a multiple regression or the logistic transformation of the posterior probability Pr(y = 1 x) in a logistic regression. In fact, these models generalize the whole family of generalized linear models $\eta$(x) = $\beta$′x, where $\eta$(x) = g($\mu$(x)) is some transformation of the regression function. We use the local scoring algorithm to estimate the functions fj(xj) nonparametrically, using a scatterplot smoother as a building block. We demonstrate the models in two different analyses: a nonparametric analysis of covariance and a logistic regression. The procedure can be used as a diagnostic tool for identifying parametric transformations of the covariates in a standard linear analysis. A variety of inferential tools have been developed to aid the analyst in assessing the relevance and significance of the estimated functions: these include confidence curves, degrees of freedom estimates, and approximate hypothesis tests. The local scoring algorithm is analogous to the iterative reweighted least squares algorithm for solving likelihood and nonlinear regression equations. At each iteration, an adjusted dependent variable is formed and an additive regression model is fit using the backfitting algorithm. The backfitting algorithm cycles through the variables and estimates each coordinated function by smoothing the partial residuals. {\textcopyright} 1976 Taylor {\&} Francis Group, LLC.},
author = {Hastie, Trevor and Tibshirani, Robert},
doi = {10.1080/01621459.1987.10478440},
file = {:C$\backslash$:/Users/boyiguo1/Downloads/2289439.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Generalized linear model,Logistic regression,Nonparametric regression,Smooth},
number = {398},
pages = {371--386},
title = {{Generalized additive models: Some applications}},
volume = {82},
year = {1987}
}

@article{Mitchell1988,
abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a “spike and slab” distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation $\sigma$, where ln($\sigma$) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter $\gamma$, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing $\gamma$, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against $\gamma$ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose y. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of “leave one out” approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods. {\textcopyright} 1976 Taylor & Francis Group, LLC.},
author = {Mitchell, T. J. and Beauchamp, J. J.},
doi = {10.1080/01621459.1988.10478694},
file = {:C\:/Users/boyiguo1/Desktop/Bayesian Variable Selection in Linear Regression.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Cross-validation,Linear models,Subset selection},
number = {404},
pages = {1023--1032},
title = {{Bayesian variable selection in linear regression}},
volume = {83},
year = {1988}
}

@article{George1993,
author = {George, Edward I. and McCulloch, Robert E.},
doi = {10.1080/01621459.1993.10476353},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/George, McCulloch - 1993 - Variable Selection via Gibbs Sampling.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {sep},
number = {423},
pages = {881--889},
title = {{Variable Selection via Gibbs Sampling}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476353},
volume = {88},
year = {1993}
}

@article{Rockova2018,
abstract = {Grounded theory analysis was applied to qualitative interviews with 25 communication professionals concerning cultural influences on crisis. This approach yielded several findings. First, public relations practitioners had difficulties in defining multiculturalism, often equating cultural diversity with communicating with Latinos. Second, interviewees saw cultural differences as just one aspect of diversity, emphasizing that age, religion, and education differences also affect corporate discourse. Third, although professionals considered culture a key element of crisis management, they did not feel prepared to handle the challenges of a multicultural crisis, nor did they report that they used culturally adjusted crisis strategies often. By integrating cultural competence and crisis management frameworks, this study provides the foundation for an in-depth understanding of crises, where scholars and professionals can pair crisis strategies with audiences' cultural expectations. Training initiatives focused on increasing levels of cultural competence can make organizations and communication professionals ready to the challenges of a global market. [ABSTRACT FROM PUBLISHER]},
author = {Ro{\v{c}}kov{\'{a}}, Veronika and George, Edward I.},
doi = {10.1080/01621459.2016.1260469},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ro{\v{c}}kov{\'{a}}, George - 2018 - The Spike-and-Slab LASSO.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {High-dimensional regression,LASSO,Penalized likelihood,Posterior concentration,Spike-and-Slab,Variable selection},
number = {521},
pages = {431--444},
publisher = {Taylor & Francis},
title = {{The Spike-and-Slab LASSO}},
url = {https://doi.org/10.1080/01621459.2016.1260469},
volume = {113},
year = {2018}
}

@article{whitehead1980,
  title={Fitting Cox's regression model to survival data using GLIM},
  author={Whitehead, John},
  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume={29},
  number={3},
  pages={268--275},
  year={1980},
  publisher={Wiley Online Library}
}

@article{Yi2012,
abstract = {Genetic and other scientific studies routinely generate very many predictor variables, which can be naturally grouped, with predictors in the same groups being highly correlated. It is desirable to incorporate the hierarchical structure of the predictor variables into generalized linear models for simultaneous variable selection and coefficient estimation. We propose two prior distributions: hierarchical Cauchy and double-exponential distributions, on coefficients in generalized linear models. The hierarchical priors include both variable-specific and group-specific tuning parameters, thereby not only adopting different shrinkage for different coefficients and different groups but also providing a way to pool the information within groups. We fit generalized linear models with the proposed hierarchical priors by incorporating flexible expectation-maximization (EM) algorithms into the standard iteratively weighted least squares as implemented in the general statistical package R. The methods are illustrated with data from an experiment to identify genetic polymorphisms for survival of mice following infection with Listeria monocytogenes. The performance of the proposed procedures is further assessed via simulation studies. The methods are implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/). {\textcopyright} 2012 De Gruyter. All rights reserved.},
author = {Yi, Nengjun and Ma, Shuangge},
doi = {10.1515/1544-6115.1803},
file = {:C$\backslash$:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yi, Ma - 2012 - Hierarchical Shrinkage Priors and Model Fitting for High-dimensional Generalized Linear Models.pdf:pdf},
issn = {1544-6115},
journal = {Statistical Applications in Genetics and Molecular Biology},
keywords = {Adaptive lasso,Bayesian inference,Generalized linear model,Genetic polymorphisms,Grouped variables,Hierarchical model,High-dimensional data,Shrinkage prior},
month = {jan},
number = {6},
pmid = {23192052},
title = {{Hierarchical Shrinkage Priors and Model Fitting for High-dimensional Generalized Linear Models}},
url = {https://www.degruyter.com/view/j/sagmb.2012.11.issue-6/1544-6115.1803/1544-6115.1803.xml},
volume = {11},
year = {2012}
}

@article{Bai2020Spline,
abstract = {We introduce a general framework for estimation and variable selection in high-dimensional Bayesian generalized additive models where the response belongs to the overdispersed exponential family. Our framework subsumes popular models such as binomial regression, Poisson regression, Gaussian regression, and negative binomial regression, and encompasses both canonical and non-canonical link functions. Our method can be implemented with a highly efficient EM algorithm, allowing us to rapidly attain estimates of the significant functions while thresholding out insignificant ones. Under mild regularity conditions, we establish posterior contraction rates and model selection consistency when the number of covariates grows at nearly exponential rate with sample size. We illustrate our method on both synthetic and real data sets.},
annote = {Bai proposed an novel high dimenional Bayesian GAM using the spike-and-slab group lasso prior. An fast fitting EM-based algorithm is proposed. Asympotoic threory, such as posterior contraction rates and model selction consistency are proved.

Even though, the algorithm is flexible to cover many non-Gaussina family in a high dimensional setting, the proposed method didn't address uncertianty quantification, i.e. calculating credile intervals etc.},
archivePrefix = {arXiv},
arxivId = {2007.07021},
author = {Bai, Ray},
eprint = {2007.07021},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bai - 2020 - A Unified Computational and Theoretical Framework for High-Dimensional Bayesian Additive Models.pdf:pdf},
keywords = {and phrases,generalized additive model,non-gaussian data,rate of con-,spike-and-slab group lasso,variable selection,vergence},
month = {jul},
pages = {1--36},
title = {{A Unified Computational and Theoretical Framework for High-Dimensional Bayesian Additive Models}},
url = {http://arxiv.org/abs/2007.07021},
year = {2020}
}

@article{Scheipl2012,
abstract = {Structured additive regression (STAR) provides a general framework for complex Gaussian and non-Gaussian regression models, with predictors comprising arbitrary combinations of nonlinear functions and surfaces, spatial effects, varying coefficients, random effects, and further regression terms. The large flexibility of STAR makes function selection a challenging and important task, aiming at (1) selecting the relevant covariates, (2) choosing an appropriate and parsimonious representation of the impact of covariates on the predictor, and (3) determining the required interactions. We propose a spike-and-slab prior structure for function selection that allows to include or exclude single coefficients as well as blocks of coefficients representing specific model terms. A novel multiplicative parameter expansion is required to obtain good mixing and convergence properties in a Markov chain Monte Carlo simulation approach and is shown to induce desirable shrinkage properties. In simulation studies and with (real) benchmark classification data, we investigate sensitivity to hyperparameter settings and compare performance to competitors. The flexibility and applicability of our approach are demonstrated in an additive piecewise exponential model with time-varying effects for right-censored survival times of intensive care patients with sepsis. Geoadditive and additive mixed logit model applications are discussed in an extensive online supplement. {\textcopyright} 2012 American Statistical Association.},
annote = {semiparametric regression model can be solved when treating as mixed models. Scheipl, Fahrmeir, Kneib (2012) proposed an new spike-and-slab structure prior, normal-mixture -of-inverse gamma (NMIG) for semiparmtric regression using MCMC. The NMIG prior is mixtrue of t-distribution with an additional coerfficients mixing vector, where the vector follows a mixtureof two Gaussian distribution.},
archivePrefix = {arXiv},
arxivId = {1105.5250},
author = {Scheipl, Fabian and Fahrmeir, Ludwig and Kneib, Thomas},
doi = {10.1080/01621459.2012.737742},
eprint = {1105.5250},
file = {:C\:/Users/boyiguo1/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheipl, Fahrmeir, Kneib - 2012 - Spike-and-slab priors for function selection in structured additive regression models.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Generalized additive mixed models,Parameter expansion,Penalized splines,Spatial regression,Stochastic search variable selection},
number = {500},
pages = {1518--1532},
title = {{Spike-and-slab priors for function selection in structured additive regression models}},
volume = {107},
year = {2012}
}