---
title: "A Scalable and Flexible Cox Proportional Hazards Model for High-Dimensional Survival Prediction and Functional Selection"
author: "Boyi Guo and Nengjun Yi"
institute: |
  | Department of Biostatistics
  | University of Alabama at Birmingham
date: "August 8th, 2022"
output: 
  beamer_presentation:
    theme: "Szeged"
    colortheme: "spruce"
    toc: FALSE
    number_section: false
    slide_level: 2
    includes:
      in_header: "preamble.tex"
classoption: "aspectratio=169"
bibliography:
  - bibfile.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
## target knits Rmds in their own session, so load libraries here.
# source("here::here(packages.R)")
```

# Background

> "It is extremely unlikely that the true (effect) function f(X) (_on the outcome_) is actually linear in X."
\hspace*{2cm}

> --- @hastie2009elements PP. 139

\begin{tcolorbox}[colback=green!5,colframe=green!40!black,title=Question]
How to model nonlinear effects for survival outcome in {\bf high-dimensional} setting?
\end{tcolorbox}

## Additive Cox Proportional Hazards Model

Following all necessary assumptions, a Cox proportional hazards model with event time $t_i$ and predictors $x_{ij}, j = 1, \dots, p$, is expressed as
$$
h(t_i) = h_0(t_i)\exp(\sum\limits^p_{j=1}B_j(x_{ij})) , \quad i = 1, \dots, n.
$$

* Spline functions
$$
B(x) = \sum\limits_{k = 1}^K \beta_k b_k(x) \equiv  \bs X^T \bs \beta
$$
  
$b_k(x)$ are the _basis functions_, possibly truncated power basis and b-spline basis. [@Wood2017] 

<!-- A _spline_ function is a piece-wise polynomial function -->

## Function Smoothing

* Smoothing penalty $\lambda \int B^{''}(X)^2dx = \lambda \bs \beta^T \bs S \bs\beta$
  * The smoothing penalty matrix $\bs S$ is known given $\bs X$
  * $\bs S$ is symmetric and positive semi-definite

* Penalized Partial Likelihood Function
$$
\hat h_0(t_i|\beta) = d_i/\sum\limits_{i^\prime \in R(t_i)} exp(X_{i^\prime}\beta).
$$
    
  * The smoothing parameter $\lambda$ is a tuning parameter, selected via cross-validation
 
 
## High-dimensional Additive Cox Model

Primary Challenges:

* Jointly model signal sparsity versus function smoothness 
* Adaptive shrinkage
* Bi-level selection that simultaneously answers
  * if a variable is predictive to the outcome, $B_j(X_j) = 0$
  * if a variable has a nonlinear relationship with the outcome, $B_j(X_j) = \beta_j X_j$

# Bayesian Hierarchical Additive Model

* Two-part spike-and-slab LASSO prior for spline functions
  * Variable selection via inclusion indicator
  * Bi-level selection accounting for effect hierarchy
  * Adaptive shrinkage via Bayesian regularization

* EM-Coordinate Descent algorithm
  * Expedited computation
  * Seamless variable selection via sparse solution


## Two-part Spike-and-slab LASSO (SSL) Prior

Follow xxx, a spline function $B(X) = \bm X^T \bm \beta$ can be decomposed to linear and nonlinear components with respect to the smoothing penalty matrix $S$
$$\bm X^T \bm \beta = X^{0} \beta + \bs X^\tp \bs \beta^\tp$$
   
* Proposed spike-and-slab LASSO prior <!-- * SSL prior for the linear coefficient and modified group SSL priors for nonlinear coefficients -->
\begin{align*}
\beta_{j} |\gamma_{j},s_0,s_1 &\sim DE(0,(1-\gamma_{j}) s_0 + \gamma_{j} s_1) \\
\beta^\tp_{jk} | \gamma^\tp_{j},s_0,s_1 &\simiid DE(0,(1-\gamma^\tp_{j}) s_0 + \gamma^\tp_{j} s_1), k = 1, \dots, K-1
\end{align*}

  * $\gamma_{j}$ controls the inclusion of linear component
  * $\gamma_{j}^\tp$ controls the inclusion of nonlinear component


## Effect Hierarchy and Adaptive Shrinkage

_Effect hierarchy_ assumes lower-order effects are more likely to be active than higher-order effects

* Structured prior on latent indicators $\gamma_j$ and $\gamma^\tp_{j}$
$$
\gamma_{j} | \theta_j \sim Bin(\gamma_{j}|1, \theta_j),\quad
\gamma_{j}^\tp | \gamma_{j}, \theta_j \sim Bin(1, \gamma_{j}\theta_j),
$$
  * Simplification via analytic integration
  $$
  \gamma_{j}^\tp | \theta_j \sim Bin(1, \theta_j^2),
  $$

* Adaptive shrinkage
$$
\theta_j \sim \text{Beta}(a,b)
$$

## EM-Cooridante Descent Algrithm

We are interested in estimating $\Theta = \{\bm \beta, \bm \theta, \phi\}$ using optimization based algorithm for scalability purpose

* Basic Ideas
  * Treat $\gamma$s as the "missing data" in the EM procedure
  * Quantify the expectation of log posterior density function of $\Theta$ with respect to $\gamma$ conditioning on $\Theta^{(t-1)}$
  * Maximize two  parts of the objective function independently

## Decomposition of Objective Function
We aim to maximize the log posterior density of $\Theta$ by averaging over all possible values of $\bm \gamma$
$$ \log f(\Theta, \bs \gamma| \textbf{y}, \textbf{X}) = Q_1(\bs \beta, \phi) + Q_2 (\bs \gamma,\bs \theta),$$

* L$_1$-penalized likelihood function of $\bs \beta, \phi$
$$Q_1 \equiv Q_1(\bs \beta, \phi) = \log f(\textbf{y}|\bs \beta, \phi) + \sum\limits_{j=1}^p\left[\log f(\beta_j|\gamma_j)+\sum\limits_{k=1}^{K_j} \log f(\beta^{\tp}_{jk}|\gamma^{\tp}_{j})\right]$$
* Posterior density of $\theta$ given data points $\gamma$s
$$Q_2 \equiv Q_2(\bs\gamma,\bs\theta) = \sum\limits_{j=1}^{p} \left[ (\gamma_j+\gamma_{j}^{\tp})\log \theta_j + (2-\gamma_j-\gamma_{j}^{\tp}) \log (1-\theta_j)\right] +  \sum\limits_{j=1}^{p}\log f(\theta_j).$$

* $Q_1$ and $Q_2$ are independent conditioning on $\gamma$s

## Summary of EM-Coordinate Descent Algorithm
* E-step
  * Formulate $E_{\bm \gamma|\Theta^{(t)}}\left[Q(\Theta, \bm \gamma)\right] = E(Q_1) + E(Q_2)$
    * $E(Q_1)$ is a $l_1$ penalized partial likelihood function of $\beta, \phi$
    * $E(Q_2)$ is a posterior density of $\theta$ given $E(\gamma)$
    * $E(Q_1)$ and $E(Q_2)$ are conditionally independent
  * Calculate $E(\gamma_{j})$, $E(\gamma^\tp_{j})$ and the penalties parameters by Bayes' theorem
* M-step: 
  * Use Coordinate Descent to fit the penalized model in $E(Q_1)$ to update $\beta, \phi$ 
  * Closed form calculation via $E(Q_2)$ to update $\theta$

# Numeric Studies
## Simulation Study
* $n_{train} = 500$, $n_{test}=1000$
* $p=4, 10, 50, 100, 200$
* Survival and censoring time follow Weibull distribution
$$
  \log \eta = (x_1 + 1)^2/5 + \exp (x_2 + 1)/25 + 3\text{sin} (x_3)/2 + (1.4x_4 + 0.5)/2
$$

* Censoring rate is controlled at \{0.15, 0.3, 0.45\}
* Splines are constructed using 10 knots
* 50 Iterations

## Comparison & Metircs
* Methods of comparison
  * Proposed model BHAM
  * Linear LASSO model as the benchmark
  * mgcv [@Wood2004]
  * COSSO [@Zhang2006GAM] and adaptive COSSO[@Storlie2011]

* Metrics
  * Out-of-sample deviance & Concordance
  
## Prediction Performance

* Linear LASSO Model performs bad in general
* Low dimensional settings:
  * mgcv performs the best 
  * BHAM performs as good as mgcv 
* High dimensional setting:
  * BHAM performs better than COSSO models as p increases and more censoring events

##  Emory Cardiovascular Biobank
::: columns
:::: column
* All-cause mortality among patents undergoing cardiac catheterization
* Sample size N=454 and number of features p=200
* 5-knot cubic spline
::::

:::: column
![](ECB_bcam_KM.pdf){height=60%}
::::
:::

# Conclusion

- A scalable and flexible Cox Model for high-dimensional survival data analysis
  - Two-part spike-and-slab LASSO prior for spline functions
    - Jointly model signal sparsity and function smoothness with adaptive regularization
    - Bi-level selection that accounts the effect hierarchy principle
  - EM-Coordinate Descent algorithm
    - Computation advantage and sparse solution


- R package: \texttt{BHAM}
  - Ancillary functions for high-dimensional formulation
  - Model summary and variable selection
  - Website via [_boyiguo1.github.io/BHAM_](https://boyiguo1.github.io/BHAM/)

# References {.allowframebreaks}
